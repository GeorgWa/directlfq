{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: utils.html\n",
    "title: Paths\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import pathlib\n",
    "if \"__file__\" in globals():#only run in the translated python file, as __file__ is not defined with ipython\n",
    "    INTABLE_CONFIG = os.path.join(pathlib.Path(__file__).parent.absolute(), \"configs\", \"intable_config.yaml\") #the yaml config is located one directory below the python library files\n",
    "    CONFIG_PATH = os.path.join(pathlib.Path(__file__).parent.absolute(), \"configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting and transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_samples_used_from_samplemap_file(samplemap_file, cond1, cond2):\n",
    "    samplemap_df = load_samplemap(samplemap_file)\n",
    "    return get_samples_used_from_samplemap_df(samplemap_df, cond1, cond2)\n",
    "\n",
    "\n",
    "def get_samples_used_from_samplemap_df(samplemap_df, cond1, cond2):\n",
    "    samples_c1 = samplemap_df[[cond1 == x for x in samplemap_df[\"condition\"]]][\"sample\"] #subset the df to the condition\n",
    "    samples_c2 = samplemap_df[[cond2 == x for x in samplemap_df[\"condition\"]]][\"sample\"]\n",
    "    return list(samples_c1), list(samples_c2)\n",
    "\n",
    "def get_all_samples_from_samplemap_df(samplemap_df):\n",
    "    return list(samplemap_df[\"sample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "\n",
    "def get_samplenames_from_input_df(data):\n",
    "    \"\"\"extracts the names of the samples of the AQ input dataframe\"\"\"\n",
    "    names = list(data.columns)\n",
    "    names.remove('protein')\n",
    "    names.remove('ion')\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "def filter_df_to_minrep(quant_df_wideformat, samples_c1, samples_c2, minrep):\n",
    "    \"\"\"filters dataframe in directlfq format such that each column has a minimum number of replicates\n",
    "    \"\"\"\n",
    "    quant_df_wideformat = quant_df_wideformat.replace(0, np.nan)\n",
    "    df_c1_minrep = quant_df_wideformat[samples_c1].dropna(thresh = minrep, axis = 0)\n",
    "    df_c2_minrep = quant_df_wideformat[samples_c2].dropna(thresh = minrep, axis = 0)\n",
    "    idxs_both = df_c1_minrep.index.intersection(df_c2_minrep.index)\n",
    "    quant_df_reduced = quant_df_wideformat.iloc[idxs_both].reset_index()\n",
    "    return quant_df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_condpairname(condpair):\n",
    "    return f\"{condpair[0]}_VS_{condpair[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_quality_score_column(acquisition_info_df):\n",
    "    if \"FG.ShapeQualityScore\" in acquisition_info_df.columns:\n",
    "        param = \"FG.ShapeQualityScore\"\n",
    "    elif \"Quantity.Quality\" in acquisition_info_df.columns:\n",
    "        param = \"Quantity.Quality\"\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "\n",
    "def make_dir_w_existcheck(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "def get_results_plot_dir_condpair(results_dir, condpair):\n",
    "    results_dir_plots = f\"{results_dir}/{condpair}_plots\"\n",
    "    make_dir_w_existcheck(results_dir_plots)\n",
    "    return results_dir_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_middle_elem(sorted_list):\n",
    "    nvals = len(sorted_list)\n",
    "    if nvals==1:\n",
    "        return sorted_list[0]\n",
    "    middle_idx = nvals//2\n",
    "    if nvals%2==1:\n",
    "        return sorted_list[middle_idx]\n",
    "    return 0.5* (sorted_list[middle_idx] + sorted_list[middle_idx-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "def get_nonna_array(array_w_nas):\n",
    "    res = []\n",
    "    isnan_arr = np.isnan(array_w_nas)\n",
    "\n",
    "    for idx in range(len(array_w_nas)):\n",
    "        sub_res = []\n",
    "        sub_array = array_w_nas[idx]\n",
    "        na_array = isnan_arr[idx]\n",
    "        for idx2 in range(len(sub_array)):\n",
    "            if not na_array[idx2]:\n",
    "               sub_res.append(sub_array[idx2])\n",
    "        res.append(np.array(sub_res))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "def get_non_nas_from_pd_df(df):\n",
    "    return {\n",
    "        pep_name: sub_vals[~np.isnan(sub_vals)] for pep_name, sub_vals in\n",
    "        zip( df.index.values, df.values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "def get_ionints_from_pd_df(df):\n",
    "    return {\n",
    "        pep_name: sub_vals for pep_name, sub_vals in\n",
    "        zip( df.index.values, df.values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def invert_dictionary(my_map):\n",
    "    inv_map = {}\n",
    "    for k, v in my_map.items():\n",
    "        inv_map[v] = inv_map.get(v, []) + [k]\n",
    "    return inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import statistics\n",
    "\n",
    "def get_z_from_p_empirical(p_emp,p2z):\n",
    "    p_rounded = np.format_float_scientific(p_emp, 1)\n",
    "    if p_rounded in p2z:\n",
    "        return p2z.get(p_rounded)\n",
    "    z = statistics.NormalDist().inv_cdf(float(p_rounded))\n",
    "    p2z[p_rounded] = z\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def count_fraction_outliers_from_expected_fc(result_df, threshold, expected_log2fc):\n",
    "    num_outliers = sum([abs(x-expected_log2fc)> threshold for x in result_df[\"log2fc\"]])\n",
    "    fraction_outliers = num_outliers/len(result_df[\"log2fc\"])\n",
    "    print(f\"{round(fraction_outliers, 2)} outliers\")\n",
    "    return fraction_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import shutil\n",
    "def create_or_replace_folder(folder):\n",
    "    if os.path.exists(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def add_mq_protein_group_ids_if_applicable_and_obtain_annotated_file(mq_file, input_type_to_use ,mq_protein_group_file, columns_to_add):\n",
    "    try:\n",
    "        input_type = _get_input_type(mq_file, input_type_to_use)\n",
    "        if (\"maxquant_evidence\" in input_type or \"maxquant_peptides\" in input_type) and (\"aq_reformat\" not in mq_file):\n",
    "            if mq_protein_group_file is None:\n",
    "                print(\"You provided a MaxQuant peptide or evidence file as input. To have the identical ProteinGroups as in the MaxQuant analysis, please provide the ProteinGroups.txt file as well.\")\n",
    "                return mq_file\n",
    "            else:\n",
    "                mq_df = load_input_file_and_de_duplicate_if_evidence(mq_file, input_type, columns_to_add)\n",
    "                id_column = determine_id_column_from_input_df(mq_df)\n",
    "                id2protein_df = create_id_to_protein_df(mq_protein_group_file, id_column)\n",
    "                annotated_mq_df = annotate_mq_df(mq_df, id2protein_df, id_column)\n",
    "                annotated_mq_filename = f\"{mq_file}.protgroup_annotated.tsv\"\n",
    "                save_annotated_mq_df(annotated_mq_df, annotated_mq_filename)\n",
    "                return annotated_mq_filename\n",
    "        else:\n",
    "            return mq_file\n",
    "    except:\n",
    "        return mq_file\n",
    "\n",
    "\n",
    "def _get_input_type(mq_file ,input_type_to_use):\n",
    "    if input_type_to_use is not None:\n",
    "        return input_type_to_use\n",
    "    else:\n",
    "        return get_input_type_and_config_dict(mq_file)[0]\n",
    "    \n",
    "\n",
    "def load_input_file_and_de_duplicate_if_evidence(input_file, input_type, columns_to_add):\n",
    "    input_df = pd.read_csv(input_file, sep = \"\\t\")\n",
    "    if \"maxquant_evidence\" in input_type:\n",
    "        subset_columns = ['id','Sequence','Modified sequence', 'Experiment','Charge', 'Raw file', 'Gene names', 'Intensity', 'Reverse', 'Potential contaminant'] + columns_to_add\n",
    "        columns_to_group_by = ['Sequence','Modified sequence', 'Experiment','Charge', 'Raw file']\n",
    "        input_df = input_df[subset_columns].set_index(columns_to_group_by)\n",
    "        input_df_grouped = input_df.groupby(columns_to_group_by).Intensity.max()\n",
    "        input_df_no_intensities = input_df.drop(columns=[\"Intensity\"])\n",
    "\n",
    "        input_df = input_df_no_intensities.merge(input_df_grouped, how= 'right', left_index=True, right_index=True).reset_index()\n",
    "        input_df = input_df.drop_duplicates(subset=columns_to_group_by)\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def create_id_to_protein_df(mq_protein_group_file, id_column):    \n",
    "    id_mapping_df = pd.read_csv(mq_protein_group_file, sep = \"\\t\", usecols=[\"Protein IDs\", id_column])\n",
    "    #apply lambda function to id column to split it into a list of ids\n",
    "    id_mapping_df[id_column] = id_mapping_df[id_column].apply(lambda x: x.split(\";\"))\n",
    "    #explode the id column\n",
    "    id_mapping_df = id_mapping_df.explode(id_column) #https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "    return id_mapping_df\n",
    "\n",
    "\n",
    "def determine_id_column_from_input_df(input_df):\n",
    "    input_file_columns = input_df.columns\n",
    "    num_cols_starting_w_intensity = sum([x.startswith(\"Intensity \") for x in input_file_columns])\n",
    "    if num_cols_starting_w_intensity>0:\n",
    "        return \"Peptide IDs\"\n",
    "    else:\n",
    "        return \"Evidence IDs\"\n",
    "\n",
    "\n",
    "def annotate_mq_df(mq_df, id2protein_df, id_column):\n",
    "    #set dtype of id to string\n",
    "    mq_df[\"id\"] = mq_df[\"id\"].astype(str)\n",
    "    id2protein_df = remove_ids_not_occurring_in_mq_df(id2protein_df, mq_df, id_column)\n",
    "    return mq_df.merge(id2protein_df, how = \"right\",  left_on = \"id\", right_on = id_column, suffixes=('', '_y'))\n",
    "\n",
    "def remove_ids_not_occurring_in_mq_df(id2protein_df, mq_df, id_column):\n",
    "    mq_df_ids = set(mq_df[\"id\"])\n",
    "    id2protein_df = id2protein_df[id2protein_df[id_column].isin(mq_df_ids)]\n",
    "    return id2protein_df\n",
    "\n",
    "def save_annotated_mq_df(annotated_mq_df, annotated_mq_file):\n",
    "    annotated_mq_df.to_csv(annotated_mq_file, sep = \"\\t\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from distutils.command.config import config\n",
    "\n",
    "\n",
    "def add_columns_to_lfq_results_table(lfq_results_df, input_file, columns_to_add):\n",
    "    input_type, config_dict, _ = get_input_type_and_config_dict(input_file)\n",
    "\n",
    "    input_file = clean_input_filename_if_necessary(input_file)\n",
    "\n",
    "    protein_column_input_table = get_protein_column_input_table(config_dict)\n",
    "    standard_columns_for_input_type = get_standard_columns_for_input_type(input_type)\n",
    "\n",
    "    all_columns = columns_to_add + [protein_column_input_table] + standard_columns_for_input_type\n",
    "    all_columns = filter_columns_to_existing_columns(all_columns, input_file)\n",
    "\n",
    "    input_df = pd.read_csv(input_file, sep=\"\\t\", usecols=all_columns).drop_duplicates(subset=protein_column_input_table)\n",
    "    lfq_results_df = lfq_results_df[[x is not None for x in lfq_results_df['protein']]]\n",
    "\n",
    "    length_before = len(lfq_results_df.index)\n",
    "    lfq_results_df_appended = pd.merge(lfq_results_df, input_df, left_on='protein', right_on=protein_column_input_table, how='left')\n",
    "    length_after = len(lfq_results_df_appended.index)\n",
    "\n",
    "    lfq_results_df_appended = lfq_results_df_appended.set_index('protein')\n",
    "    \n",
    "\n",
    "    assert length_before == length_after\n",
    "    return lfq_results_df_appended\n",
    "\n",
    "def clean_input_filename_if_necessary(input_file):\n",
    "    if \"aq_reformat.tsv\" in input_file:\n",
    "        input_file = get_original_file_from_aq_reformat(input_file)\n",
    "    return input_file\n",
    "\n",
    "def get_protein_column_input_table(config_dict):\n",
    "    return config_dict[\"protein_cols\"][0]\n",
    "\n",
    "def get_standard_columns_for_input_type(input_type):\n",
    "    \n",
    "    if 'maxquant' in input_type:\n",
    "        return [\"Gene names\"]\n",
    "    elif 'diann' in input_type:\n",
    "        return [\"Protein.Names\", \"Genes\"]\n",
    "    elif 'spectronaut' in input_type:\n",
    "        return ['PG.Genes']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def filter_columns_to_existing_columns(columns, input_file):\n",
    "    existing_columns =  pd.read_csv(input_file, sep='\\t', nrows=1).columns\n",
    "    return [x for x in columns if x in existing_columns]\n",
    "\n",
    "\n",
    "\n",
    "#function that shows the differing rows between two dataframes\n",
    "def show_diff(df1, df2):\n",
    "    return df1.merge(df2, indicator=True, how='outer').loc[lambda x : x['_merge']!='both']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def write_chunk_to_file(chunk, filepath ,write_header):\n",
    "    \"\"\"write chunk of pandas dataframe to a file\"\"\"\n",
    "    chunk.to_csv(filepath, header=write_header, mode='a', sep = \"\\t\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def index_and_log_transform_input_df(data_df):\n",
    "    data_df = data_df.set_index([\"protein\", \"ion\"])\n",
    "    return np.log2(data_df.replace(0, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def remove_allnan_rows_input_df(data_df):\n",
    "    return data_df.dropna(axis = 0, how = 'all')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Parsers\n",
    "The directlfq pipeline is run using a generic wide-table input format, as specified in the documentation. The following parsers convert long format tables as provided e.g. by Spectronaut or DIA-NN into this generic format. The configuration for the parsers is set by a yaml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert long format to wide format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse .yaml file\n",
    "The relevant parameters for reading and reformatting the long table are stored in the \"intable_config.yaml\" file. The functions below are for reading and reformating the config info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "def get_relevant_columns(protein_cols, ion_cols, sample_ID, quant_ID, filter_dict):\n",
    "    filtcols = []\n",
    "    for filtconf in filter_dict.values():\n",
    "        filtcols.append(filtconf.get('param'))\n",
    "    relevant_cols = protein_cols + ion_cols + [sample_ID] + [quant_ID] + filtcols\n",
    "    relevant_cols = list(set(relevant_cols)) # to remove possible redudancies\n",
    "    return relevant_cols\n",
    "\n",
    "\n",
    "def get_relevant_columns_config_dict(config_typedict):\n",
    "    filtcols = []\n",
    "    dict_ioncols = []\n",
    "    for filtconf in config_typedict.get('filters', {}).values():\n",
    "        filtcols.append(filtconf.get('param'))\n",
    "\n",
    "    if 'ion_hierarchy' in config_typedict.keys():\n",
    "        for headr in config_typedict.get('ion_hierarchy').values():\n",
    "            ioncols = list(itertools.chain.from_iterable(headr.get(\"mapping\").values()))\n",
    "            dict_ioncols.extend(ioncols)\n",
    "\n",
    "    quant_ids = get_quant_ids_from_config_dict(config_typedict)\n",
    "    sample_ids = get_sample_ids_from_config_dict(config_typedict)\n",
    "    channel_ids = get_channel_ids_from_config_dict(config_typedict)\n",
    "    relevant_cols = config_typedict.get(\"protein_cols\") + config_typedict.get(\"ion_cols\", []) + sample_ids + quant_ids + filtcols + dict_ioncols + channel_ids\n",
    "    relevant_cols = list(set(relevant_cols)) # to remove possible redudancies\n",
    "    return relevant_cols\n",
    "\n",
    "def get_quant_ids_from_config_dict(config_typedict):\n",
    "    quantID = config_typedict.get(\"quant_ID\")\n",
    "    if type(quantID) ==type(\"string\"):\n",
    "        return [config_typedict.get(\"quant_ID\")]\n",
    "    if quantID == None:\n",
    "        return[]\n",
    "    else:\n",
    "        return list(config_typedict.get(\"quant_ID\").values())\n",
    "\n",
    "def get_sample_ids_from_config_dict(config_typedict):\n",
    "    sampleID = config_typedict.get(\"sample_ID\")\n",
    "    if type(sampleID) ==type(\"string\"):\n",
    "        return [config_typedict.get(\"sample_ID\")]\n",
    "    if sampleID == None:\n",
    "        return []\n",
    "    else:\n",
    "        return config_typedict.get(\"sample_ID\")\n",
    "\n",
    "def get_channel_ids_from_config_dict(config_typedict):\n",
    "    return config_typedict.get(\"channel_ID\", [])\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_yaml):\n",
    "    with open(config_yaml, 'r') as stream:\n",
    "        config_all = yaml.safe_load(stream)\n",
    "    return config_all\n",
    "\n",
    "def get_type2relevant_cols(config_all):\n",
    "    type2relcols = {}\n",
    "    for type in config_all.keys():\n",
    "        config_typedict = config_all.get(type)\n",
    "        relevant_cols = get_relevant_columns_config_dict(config_typedict)\n",
    "        type2relcols[type] = relevant_cols\n",
    "    return type2relcols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_input(filter_dict, input):\n",
    "    if filter_dict == None:\n",
    "        return input\n",
    "    for filtname,filterconf in filter_dict.items():\n",
    "        param = filterconf.get('param')\n",
    "        comparator = filterconf.get('comparator')\n",
    "        value = filterconf.get('value')\n",
    "\n",
    "        if comparator not in [\">\",\">=\", \"<\", \"<=\", \"==\", \"!=\"]:\n",
    "            raise TypeError(f\"cannot identify the filter comparator of {filtname} given in the longtable config yaml!\")\n",
    "\n",
    "        if comparator==\"==\":\n",
    "            input = input[input[param] ==value]\n",
    "            continue\n",
    "        try:\n",
    "            input = input.astype({f\"{param}\" : \"float\"})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if comparator==\">\":\n",
    "            input = input[input[param].astype(type(value)) >value]\n",
    "\n",
    "        if comparator==\">=\":\n",
    "            input = input[input[param].astype(type(value)) >=value]\n",
    "\n",
    "        if comparator==\"<\":\n",
    "            input = input[input[param].astype(type(value)) <value]\n",
    "\n",
    "        if comparator==\"<=\":\n",
    "            input = input[input[param].astype(type(value)) <=value]\n",
    "\n",
    "        if comparator==\"!=\":\n",
    "            input = input[input[param].astype(type(value)) !=value]\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_protein_and_ion_cols(input_df, config_dict):\n",
    "    protein_cols =  config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    input_df['protein'] = input_df.loc[:, protein_cols].astype('string').sum(axis=1)\n",
    "    input_df['ion'] = input_df.loc[:, ion_cols].astype('string').sum(axis=1)\n",
    "    input_df = input_df.rename(columns = {config_dict.get('quant_ID') : \"quant_val\"})\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "def merge_protein_cols_and_ion_dict(input_df, config_dict):\n",
    "    \"\"\"[summary]\n",
    "    \n",
    "    Args:\n",
    "        input_df ([pandas dataframe]): longtable containing peptide intensity data\n",
    "        confid_dict ([dict[String[]]]): nested dict containing the parse information. derived from yaml file\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframe: longtable with newly assigned \"protein\" and \"ion\" columns\n",
    "    \"\"\"\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_hierarchy = config_dict.get(\"ion_hierarchy\")\n",
    "    splitcol2sep = config_dict.get('split_cols')\n",
    "    quant_id_dict = config_dict.get('quant_ID')\n",
    "\n",
    "    ion_dfs = []\n",
    "    input_df['protein'] = input_df.loc[:, protein_cols].astype('string').sum(axis=1)\n",
    "\n",
    "    input_df = input_df.drop(columns = [x for x in protein_cols if x!='protein'])\n",
    "    for hierarchy_type in ion_hierarchy.keys():\n",
    "        df_subset = input_df.copy()\n",
    "        ion_hierarchy_local = ion_hierarchy.get(hierarchy_type).get(\"order\")\n",
    "        ion_headers_merged, ion_headers_grouped = get_ionname_columns(ion_hierarchy.get(hierarchy_type).get(\"mapping\"), ion_hierarchy_local) #ion headers merged is just a helper to select all relevant rows, ionheaders grouped contains the sets of ionstrings to be merged into a list eg [[SEQ, MOD], [CH]]\n",
    "        quant_columns = get_quantitative_columns(df_subset, hierarchy_type, config_dict, ion_headers_merged)\n",
    "        headers = list(set(ion_headers_merged + quant_columns + ['protein']))\n",
    "        if \"sample_ID\" in config_dict.keys():\n",
    "            headers+=[config_dict.get(\"sample_ID\")]\n",
    "        df_subset = df_subset[headers].drop_duplicates()\n",
    "\n",
    "        if splitcol2sep is not None:\n",
    "            if quant_columns[0] in splitcol2sep.keys(): #in the case that quantitative values are stored grouped in one column (e.g. msiso1,msiso2,msiso3, etc.), reformat accordingly\n",
    "                df_subset = split_extend_df(df_subset, splitcol2sep)\n",
    "            ion_headers_grouped = adapt_headers_on_extended_df(ion_headers_grouped, splitcol2sep)\n",
    "\n",
    "        #df_subset = df_subset.set_index(quant_columns)\n",
    "\n",
    "        df_subset = add_merged_ionnames(df_subset, ion_hierarchy_local, ion_headers_grouped, quant_id_dict, hierarchy_type)\n",
    "        ion_dfs.append(df_subset)\n",
    "    input_df = pd.concat(ion_dfs, ignore_index=True)\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def get_quantitative_columns(input_df, hierarchy_type, config_dict, ion_headers_merged):\n",
    "    naming_columns = ion_headers_merged + ['protein']\n",
    "    if config_dict.get(\"format\") == 'longtable':\n",
    "        quantcol = config_dict.get(\"quant_ID\").get(hierarchy_type)\n",
    "        return [quantcol]\n",
    "\n",
    "    if config_dict.get(\"format\") == 'widetable':\n",
    "        quantcolumn_candidates = [x for x in input_df.columns if x not in naming_columns]\n",
    "        if \"quant_pre_or_suffix\" in config_dict.keys():\n",
    "            return [x for x in quantcolumn_candidates if x.startswith(config_dict.get(\"quant_pre_or_suffix\")) or x.endswith(config_dict.get(\"quant_pre_or_suffix\"))] # in the case that the quantitative columns have a prefix (like \"Intensity \" in MQ peptides.txt), only columns with the prefix are filtered\n",
    "        else:\n",
    "            return quantcolumn_candidates #in this case, we assume that all non-ionname/proteinname columns are quantitative columns\n",
    "\n",
    "\n",
    "def get_ionname_columns(ion_dict, ion_hierarchy_local):\n",
    "    ion_headers_merged = []\n",
    "    ion_headers_grouped = []\n",
    "    for lvl in ion_hierarchy_local:\n",
    "        vals = ion_dict.get(lvl)\n",
    "        ion_headers_merged.extend(vals)\n",
    "        ion_headers_grouped.append(vals)\n",
    "    return ion_headers_merged, ion_headers_grouped\n",
    "\n",
    "\n",
    "def adapt_headers_on_extended_df(ion_headers_grouped, splitcol2sep):\n",
    "    #in the case that one column has been split, we need to designate the \"naming\" column\n",
    "    ion_headers_grouped_copy = copy.deepcopy(ion_headers_grouped)\n",
    "    for vals in ion_headers_grouped_copy:\n",
    "        if splitcol2sep is not None:\n",
    "            for idx in range(len(vals)):\n",
    "                if vals[idx] in splitcol2sep.keys():\n",
    "                    vals[idx] = vals[idx] + \"_idxs\"\n",
    "    return ion_headers_grouped_copy\n",
    "\n",
    "def split_extend_df(input_df, splitcol2sep, value_threshold=10):\n",
    "    \"\"\"reformats data that is stored in a condensed way in a single column. For example isotope1_intensity;isotope2_intensity etc. in Spectronaut\n",
    "\n",
    "    Args:\n",
    "        input_df ([type]): [description]\n",
    "        splitcol2sep ([type]): [description]\n",
    "        value_threshold([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        Pandas Dataframe: Pandas dataframe with the condensed items expanded to long format\n",
    "    \"\"\"\n",
    "    if splitcol2sep==None:\n",
    "        return input_df\n",
    "\n",
    "    for split_col, separator in splitcol2sep.items():\n",
    "        idx_name = f\"{split_col}_idxs\"\n",
    "        split_col_series = input_df[split_col].str.split(separator)\n",
    "        input_df = input_df.drop(columns = [split_col])\n",
    "\n",
    "        input_df[idx_name] = [list(range(len(x))) for x in split_col_series]\n",
    "        exploded_input = input_df.explode(idx_name)\n",
    "        exploded_split_col_series = split_col_series.explode()\n",
    "\n",
    "        exploded_input[split_col] = exploded_split_col_series.replace('', 0) #the column with the intensities has to come after to column with the idxs\n",
    "\n",
    "        exploded_input = exploded_input.astype({split_col: float})\n",
    "        exploded_input = exploded_input[exploded_input[split_col]>value_threshold]\n",
    "        #exploded_input = exploded_input.rename(columns = {'var1': split_col})\n",
    "    return exploded_input\n",
    "\n",
    "\n",
    "\n",
    "def add_merged_ionnames(df_subset, ion_hierarchy_local, ion_headers_grouped, quant_id_dict, hierarchy_type):\n",
    "    \"\"\"puts together the hierarchical ion names as a column in a given input dataframe\"\"\"\n",
    "    all_ion_headers = list(itertools.chain.from_iterable(ion_headers_grouped))\n",
    "    columns_to_index = [x for x in df_subset.columns if x not in all_ion_headers]\n",
    "    df_subset = df_subset.set_index(columns_to_index)\n",
    "\n",
    "    rows = df_subset[all_ion_headers].to_numpy()\n",
    "    ions = []\n",
    "\n",
    "    for row in rows: #iterate through dataframe\n",
    "        count = 0\n",
    "        ionstring = \"\"\n",
    "        for lvl_idx in range(len(ion_hierarchy_local)):\n",
    "            ionstring += f\"{ion_hierarchy_local[lvl_idx]}\"\n",
    "            for sublvl in ion_headers_grouped[lvl_idx]:\n",
    "                ionstring+= f\"_{row[count]}_\"\n",
    "                count+=1\n",
    "        ions.append(ionstring)\n",
    "    df_subset['ion'] = ions\n",
    "    df_subset = df_subset.reset_index()\n",
    "    if quant_id_dict!= None:\n",
    "        df_subset = df_subset.rename(columns = {quant_id_dict.get(hierarchy_type) : \"quant_val\"})\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os.path\n",
    "def reformat_and_write_longtable_according_to_config(input_file, outfile_name, config_dict_for_type, sep = \"\\t\",decimal = \".\", enforce_largefile_processing = False, chunksize =1000_000):\n",
    "    \"\"\"Reshape a long format proteomics results table (e.g. Spectronaut or DIA-NN) to a wide format table.\n",
    "    :param file input_file: long format proteomic results table\n",
    "    :param string input_type: the configuration key stored in the config file (e.g. \"diann_precursor\")\n",
    "    \"\"\"\n",
    "    filesize = os.path.getsize(input_file)/(1024**3) #size in gigabyte\n",
    "    file_is_large = (filesize>10 and str(input_file).endswith(\".zip\")) or filesize>50 or enforce_largefile_processing\n",
    "\n",
    "    if file_is_large:\n",
    "        tmpfile_large = f\"{input_file}.tmp.longformat.columnfilt.tsv\" #only needed when file is large\n",
    "        #remove potential leftovers from previous processings\n",
    "        if os.path.exists(tmpfile_large):\n",
    "            os.remove(tmpfile_large)\n",
    "        if os.path.exists(outfile_name):\n",
    "            os.remove(outfile_name)\n",
    "    \n",
    "    relevant_cols = get_relevant_columns_config_dict(config_dict_for_type)\n",
    "    input_df_it = pd.read_csv(input_file, sep = sep, decimal=decimal, usecols = relevant_cols, encoding ='latin1', chunksize = chunksize)\n",
    "    input_df_list = []\n",
    "    header = True\n",
    "    for input_df_subset in input_df_it:\n",
    "        input_df_subset = adapt_subtable(input_df_subset, config_dict_for_type)\n",
    "        if file_is_large:\n",
    "            write_chunk_to_file(input_df_subset,tmpfile_large, header)\n",
    "        else:\n",
    "            input_df_list.append(input_df_subset)\n",
    "        header = False\n",
    "        \n",
    "    if file_is_large:\n",
    "        process_with_dask(tmpfile_columnfilt=tmpfile_large , outfile_name = outfile_name, config_dict_for_type=config_dict_for_type)\n",
    "    else:\n",
    "        input_df = pd.concat(input_df_list)\n",
    "        input_reshaped = reshape_input_df(input_df, config_dict_for_type)\n",
    "        input_reshaped.to_csv(outfile_name, sep = \"\\t\", index = None)\n",
    "    \n",
    "\n",
    "def adapt_subtable(input_df_subset, config_dict):\n",
    "    input_df_subset = filter_input(config_dict.get(\"filters\", {}), input_df_subset)\n",
    "    if \"ion_hierarchy\" in config_dict.keys():\n",
    "        return merge_protein_cols_and_ion_dict(input_df_subset, config_dict)\n",
    "    else:\n",
    "        return merge_protein_and_ion_cols(input_df_subset, config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import shutil \n",
    "\n",
    "def process_with_dask(*, tmpfile_columnfilt, outfile_name, config_dict_for_type):\n",
    "    df = dd.read_csv(tmpfile_columnfilt, sep = \"\\t\")\n",
    "    allcols = df[config_dict_for_type.get(\"sample_ID\")].drop_duplicates().compute() # the columns of the output table are the sample IDs\n",
    "    allcols = extend_sample_allcolumns_for_plexdia_case(allcols_samples=allcols, config_dict_for_type=config_dict_for_type)\n",
    "    allcols = ['protein', 'ion'] + sorted(allcols)\n",
    "    df = df.set_index('protein')\n",
    "    sorted_filedir = f\"{tmpfile_columnfilt}_sorted\"\n",
    "    df.to_csv(sorted_filedir, sep = \"\\t\")\n",
    "    #now the files are sorted and can be pivoted chunkwise (multiindex pivoting at the moment not possible in dask)\n",
    "    files_dask = glob.glob(f\"{sorted_filedir}/*part\")\n",
    "    header = True\n",
    "    for file in files_dask:\n",
    "        input_df = pd.read_csv(file, sep = \"\\t\")\n",
    "        if len(input_df.index) <2:\n",
    "            continue\n",
    "        input_reshaped = reshape_input_df(input_df, config_dict_for_type)\n",
    "        input_reshaped = sort_and_add_columns(input_reshaped, allcols)\n",
    "        write_chunk_to_file(input_reshaped, outfile_name, header)\n",
    "        header = False\n",
    "    os.remove(tmpfile_columnfilt)\n",
    "    shutil.rmtree(sorted_filedir)\n",
    "\n",
    "def reshape_input_df(input_df, config_dict):\n",
    "    input_df = input_df.astype({'quant_val': 'float'})\n",
    "    input_df = adapt_input_df_columns_in_case_of_plexDIA(input_df=input_df, config_dict_for_type=config_dict)\n",
    "    input_reshaped = pd.pivot_table(input_df, index = ['protein', 'ion'], columns = config_dict.get(\"sample_ID\"), values = 'quant_val', fill_value=0)\n",
    "\n",
    "    input_reshaped = input_reshaped.reset_index()\n",
    "    return input_reshaped\n",
    "\n",
    "\n",
    "def sort_and_add_columns(input_reshaped, allcols):\n",
    "    missing_cols = set(allcols) - set(input_reshaped.columns)\n",
    "    input_reshaped[list(missing_cols)] = 0\n",
    "    input_reshaped = input_reshaped[allcols]\n",
    "    return input_reshaped\n",
    "\n",
    "\n",
    "def extend_sample_allcolumns_for_plexdia_case(allcols_samples, config_dict_for_type):\n",
    "    if is_plexDIA_table(config_dict_for_type):\n",
    "        new_allcols = []\n",
    "        channels = ['mTRAQ-n-0', 'mTRAQ-n-4', 'mTRAQ-n-8']\n",
    "        for channel in channels:\n",
    "            for sample in allcols_samples:\n",
    "                new_allcols.append(merge_channel_and_sample_string(sample, channel))\n",
    "        return new_allcols\n",
    "    else:\n",
    "        return allcols_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#PLEXDIA case\n",
    "\n",
    "def adapt_input_df_columns_in_case_of_plexDIA(input_df,config_dict_for_type):\n",
    "    if is_plexDIA_table(config_dict_for_type):\n",
    "        input_df = extend_sampleID_column_for_plexDIA_case(input_df, config_dict_for_type)\n",
    "        input_df = set_mtraq_reduced_ion_column_into_dataframe(input_df)\n",
    "        return input_df\n",
    "    else:\n",
    "        return input_df\n",
    "\n",
    "\n",
    "def extend_sampleID_column_for_plexDIA_case(input_df,config_dict_for_type):\n",
    "    channels_per_peptide = parse_channel_from_peptide_column(input_df)\n",
    "    return merge_sample_id_and_channels(input_df, channels_per_peptide, config_dict_for_type)\n",
    "\n",
    "\n",
    "def set_mtraq_reduced_ion_column_into_dataframe(input_df):\n",
    "    new_ions = remove_mtraq_modifications_from_ion_ids(input_df['ion'])\n",
    "    input_df['ion'] = new_ions\n",
    "    return input_df\n",
    "\n",
    "def remove_mtraq_modifications_from_ion_ids(ions):\n",
    "    new_ions = []\n",
    "    all_mtraq_tags = [\"(mTRAQ-K-0)\", \"(mTRAQ-K-4)\", \"(mTRAQ-K-8)\", \"(mTRAQ-n-0)\", \"(mTRAQ-n-4)\", \"(mTRAQ-n-8)\"]\n",
    "    for ion in ions:\n",
    "        for tag in all_mtraq_tags:\n",
    "            ion = ion.replace(tag, \"\")\n",
    "        new_ions.append(ion)\n",
    "    return new_ions\n",
    "\n",
    "\n",
    "def is_plexDIA_table(config_dict_for_type):\n",
    "    return config_dict_for_type.get('channel_ID') == ['Channel.0', 'Channel.4', 'Channel.8']\n",
    "\n",
    "\n",
    "import re\n",
    "def parse_channel_from_peptide_column(input_df):\n",
    "    channels = []\n",
    "    for pep in input_df['Modified.Sequence']:\n",
    "        pattern = \"(.*)(\\(mTRAQ-n-.\\))(.*)\"\n",
    "        matched = re.match(pattern, pep)\n",
    "        num_appearances = pep.count(\"mTRAQ-n-\")\n",
    "        if matched and num_appearances==1:\n",
    "            channels.append(matched.group(2))\n",
    "        else:\n",
    "            channels.append(\"NA\")\n",
    "    return channels\n",
    "\n",
    "def merge_sample_id_and_channels(input_df, channels, config_dict_for_type):\n",
    "    sample_id = config_dict_for_type.get(\"sample_ID\")\n",
    "    sample_ids = list(input_df[sample_id])\n",
    "    input_df[sample_id] = [merge_channel_and_sample_string(sample_ids[idx], channels[idx]) for idx in range(len(sample_ids))]\n",
    "    return input_df\n",
    "            \n",
    "def merge_channel_and_sample_string(sample, channel):\n",
    "    return f\"{sample}_{channel}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def test_remove_remove_mtraq_modifications_from_ion_ids():\n",
    "    ions = [\"SEQ_IAVLLAK_MOD_(mTRAQ-n-0)IAVLLAK(mTRAQ-K-0)\", \"SEQ_IAVLLAK_MOD_(mTRAQ-n-0)IAVLLAK(mTRAQ-K-0)_CHARGE_1_FRGION_2_\",  \"SEQ_IAVLLAK_MOD_(mTRAQ-n-0)I(mTRAQ-n-0)AVL(mTRAQ-K-0)LAK(mTRAQ-K-0)_CHARGE_1_FRGION_2_\"]\n",
    "    new_ions = remove_mtraq_modifications_from_ion_ids(ions)\n",
    "    for ion in new_ions:\n",
    "        assert 'mTRAQ' not in ion\n",
    "test_remove_remove_mtraq_modifications_from_ion_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reformat_and_write_wideformat_table(peptides_tsv, outfile_name, config_dict):\n",
    "    input_df = pd.read_csv(peptides_tsv,sep=\"\\t\", encoding ='latin1')\n",
    "    filter_dict = config_dict.get(\"filters\")\n",
    "    protein_cols = config_dict.get(\"protein_cols\")\n",
    "    ion_cols = config_dict.get(\"ion_cols\")\n",
    "    input_df = filter_input(filter_dict, input_df)\n",
    "    #input_df = merge_protein_and_ion_cols(input_df, config_dict)\n",
    "    input_df = merge_protein_cols_and_ion_dict(input_df, config_dict)\n",
    "    if 'quant_pre_or_suffix' in config_dict.keys():\n",
    "        quant_pre_or_suffix = config_dict.get('quant_pre_or_suffix')\n",
    "        headers = ['protein', 'ion'] + list(filter(lambda x: x.startswith(quant_pre_or_suffix) or x.endswith(quant_pre_or_suffix), input_df.columns))\n",
    "        input_df = input_df[headers]\n",
    "        input_df = input_df.rename(columns = lambda x : x.replace(quant_pre_or_suffix, \"\"))\n",
    "\n",
    "    #input_df = input_df.reset_index()\n",
    "    \n",
    "    input_df.to_csv(outfile_name, sep = '\\t', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_mq_peptides_table(peptides_tsv, pepheader = \"Sequence\", protheader = \"Leading razor protein\"):\n",
    "    peps = pd.read_csv(peptides_tsv,sep=\"\\t\", encoding ='latin1')\n",
    "    peps = peps[peps[\"Reverse\"] != \"+\"]\n",
    "    peps = peps[peps[\"Potential contaminant\"] != \"+\"]\n",
    "    if pepheader != None:\n",
    "        peps = peps.rename(columns = {pepheader : \"ion\"})\n",
    "    if protheader != None:\n",
    "        peps = peps.rename(columns = {protheader: \"protein\"})\n",
    "    headers = ['protein', 'ion'] + list(filter(lambda x: x.startswith(\"Intensity \"), peps.columns))\n",
    "    peps = peps[headers]\n",
    "    peps = peps.rename(columns = lambda x : x.replace(\"Intensity \", \"\"))\n",
    "\n",
    "    return peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for already processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "def check_for_processed_runs_in_results_folder(results_folder):\n",
    "    contained_condpairs = []\n",
    "    folder_files = os.listdir(results_folder)\n",
    "    result_files = list(filter(lambda x: \"results.tsv\" in x ,folder_files))\n",
    "    for result_file in result_files:\n",
    "        res_name = result_file.replace(\".results.tsv\", \"\")\n",
    "        if ((f\"{res_name}.normed.tsv\" in folder_files) & (f\"{res_name}.results.ions.tsv\" in folder_files)):\n",
    "            contained_condpairs.append(res_name)\n",
    "    return contained_condpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "def import_data(input_file, input_type_to_use = None, samples_subset = None, results_dir = None):\n",
    "    \"\"\"\n",
    "    Function to import peptide level data. Depending on available columns in the provided file,\n",
    "    the function identifies the type of input used (e.g. Spectronaut, MaxQuant, DIA-NN), reformats if necessary\n",
    "    and returns a generic wide-format dataframe\n",
    "    :param file input_file: quantified peptide/ion -level data\n",
    "    :param file results_folder: the folder where the directlfq outputs are stored\n",
    "    \"\"\"\n",
    "\n",
    "    samples_subset = add_ion_protein_headers_if_applicable(samples_subset)\n",
    "    if \"aq_reformat\" in input_file:\n",
    "        file_to_read = input_file\n",
    "    else:\n",
    "        file_to_read = reformat_and_save_input_file(input_file=input_file, input_type_to_use=input_type_to_use)\n",
    "    \n",
    "    input_reshaped = pd.read_csv(file_to_read, sep = \"\\t\", encoding = 'latin1', usecols=samples_subset)\n",
    "    input_reshaped = input_reshaped.drop_duplicates(subset='ion')\n",
    "    return input_reshaped\n",
    "\n",
    "\n",
    "def reformat_and_save_input_file(input_file, input_type_to_use = None):\n",
    "    \n",
    "    input_type, config_dict_for_type, sep = get_input_type_and_config_dict(input_file, input_type_to_use)\n",
    "    print(f\"using input type {input_type}\")\n",
    "    format = config_dict_for_type.get('format')\n",
    "    outfile_name = f\"{input_file}.{input_type}.aq_reformat.tsv\"\n",
    "\n",
    "    if format == \"longtable\":\n",
    "        reformat_and_write_longtable_according_to_config(input_file, outfile_name,config_dict_for_type, sep = sep)\n",
    "    elif format == \"widetable\":\n",
    "        reformat_and_write_wideformat_table(input_file, outfile_name, config_dict_for_type)\n",
    "    else:\n",
    "        raise Exception('Format not recognized!')\n",
    "    return outfile_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_ion_protein_headers_if_applicable(samples_subset):\n",
    "    if samples_subset is not None:\n",
    "        return samples_subset + [\"ion\", \"protein\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import pathlib\n",
    "\n",
    "def get_input_type_and_config_dict(input_file, input_type_to_use = None):\n",
    "    #parse the type of input (e.g. Spectronaut Fragion+MS1Iso) out of the input file\n",
    "\n",
    "\n",
    "    config_dict = load_config(INTABLE_CONFIG)\n",
    "    type2relevant_columns = get_type2relevant_cols(config_dict)\n",
    "\n",
    "    if \"aq_reformat.tsv\" in input_file:\n",
    "        input_file = get_original_file_from_aq_reformat(input_file)\n",
    "\n",
    "    filename = str(input_file)\n",
    "    if '.csv' in filename:\n",
    "        sep=','\n",
    "    if '.tsv' in filename:\n",
    "        sep='\\t'\n",
    "    if '.txt' in filename:\n",
    "        sep='\\t'\n",
    "\n",
    "    if 'sep' not in locals():\n",
    "        raise TypeError(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {input_file}! Your filename has to contain one of these extensions. Please modify your file name accordingly.\")\n",
    "\n",
    "\n",
    "\n",
    "    uploaded_data_columns = set(pd.read_csv(input_file, sep=sep, nrows=1, encoding ='latin1').columns)\n",
    "\n",
    "    for input_type in type2relevant_columns.keys():\n",
    "        if (input_type_to_use is not None) and (input_type!=input_type_to_use):\n",
    "            continue\n",
    "        relevant_columns = type2relevant_columns.get(input_type)\n",
    "        relevant_columns = [x for x in relevant_columns if x] #filter None values\n",
    "        if set(relevant_columns).issubset(uploaded_data_columns):\n",
    "            config_dict_type =  config_dict.get(input_type)\n",
    "            return input_type, config_dict_type, sep\n",
    "    raise TypeError(\"format not specified in intable_config.yaml!\")\n",
    "\n",
    "import re\n",
    "def get_original_file_from_aq_reformat(input_file):\n",
    "    matched = re.match(\"(.*)(\\..*\\.)(aq_reformat\\.tsv)\",input_file)\n",
    "    return matched.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "def test_get_original_file_from_aq_reformat():\n",
    "    assert get_original_file_from_aq_reformat(\"yeast_report_fastafiltered.tsv.some.oth.erstuff.spectronaut_fragion_isotopes.aq_reformat.tsv\") == \"yeast_report_fastafiltered.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def import_config_dict():\n",
    "    config_dict = load_config(INTABLE_CONFIG)\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_samplemap(samplemap_file):\n",
    "    file_ext = os.path.splitext(samplemap_file)[-1]\n",
    "    if file_ext=='.csv':\n",
    "        sep=','\n",
    "    if (file_ext=='.tsv') | (file_ext=='.txt'):\n",
    "        sep='\\t'\n",
    "\n",
    "    if 'sep' not in locals():\n",
    "        print(f\"neither of the file extensions (.tsv, .csv, .txt) detected for file {samplemap_file}! Trying with tab separation. In the case that it fails, please add the appropriate extension to your file name.\")\n",
    "        sep = \"\\t\"\n",
    "\n",
    "    return pd.read_csv(samplemap_file, sep = sep, encoding ='latin1', dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_loaded_tables(data_df, samplemap_df):\n",
    "    \"\"\"\n",
    "    Integrates information from the peptide/ion data and the samplemap, selects the relevant columns and log2 transforms intensities.\n",
    "    \"\"\"\n",
    "    samplemap_df = samplemap_df[samplemap_df[\"condition\"]!=\"\"] #remove rows that have no condition entry\n",
    "    filtvec_not_in_data = [(x in data_df.columns) for x in samplemap_df[\"sample\"]] #remove samples that are not in the dataframe\n",
    "    samplemap_df = samplemap_df[filtvec_not_in_data]\n",
    "    headers = ['protein'] + samplemap_df[\"sample\"].to_list()\n",
    "    data_df = data_df.set_index(\"ion\")\n",
    "    for sample in samplemap_df[\"sample\"]:\n",
    "        data_df[sample] = np.log2(data_df[sample].replace(0, np.nan))\n",
    "    return data_df[headers], samplemap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "#| export\n",
    "class LongTableReformater():\n",
    "    \"\"\"Generic class to reformat tabular files in chunks. For the specific cases you can inherit the class and specify reformat and iterate function\n",
    "    \"\"\"\n",
    "    def __init__(self, input_file):\n",
    "        self._input_file = input_file\n",
    "        self._reformatting_function = None\n",
    "        self._iterator_function = self.__initialize_df_iterator__\n",
    "        self._concat_list = []\n",
    "\n",
    "    def reformat_and_load_acquisition_data_frame(self):\n",
    "\n",
    "        input_df_it = self._iterator_function()\n",
    "        \n",
    "        input_df_list = []\n",
    "        for input_df_subset in input_df_it:\n",
    "            input_df_subset = self._reformatting_function(input_df_subset)\n",
    "            input_df_list.append(input_df_subset)\n",
    "        input_df = pd.concat(input_df_list)\n",
    "        \n",
    "        return input_df\n",
    "\n",
    "    def reformat_and_save_acquisition_data_frame(self, output_file):\n",
    "        \n",
    "        input_df_it = self._iterator_function()\n",
    "        write_header = True\n",
    "        \n",
    "        for input_df_subset in input_df_it:\n",
    "            input_df_subset = self._reformatting_function(input_df_subset)\n",
    "            self.__write_reformatted_df_to_file__(input_df_subset, output_file, write_header)\n",
    "            write_header = False\n",
    "\n",
    "    def __initialize_df_iterator__(self):\n",
    "        return pd.read_csv(self._input_file, sep = \"\\t\", encoding ='latin1', chunksize=1000000)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __write_reformatted_df_to_file__(reformatted_df, filepath ,write_header):\n",
    "        reformatted_df.to_csv(filepath, header=write_header, mode='a', sep = \"\\t\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "class AcquisitionTableHandler():\n",
    "    def __init__(self, results_dir, samples):\n",
    "        self._table_infos = AcquisitionTableInfo(results_dir=results_dir)\n",
    "        self._header_infos = AcquisitionTableHeaders(self._table_infos)\n",
    "        self._samples = self.__reformat_samples_if_necessary(samples)\n",
    "    \n",
    "    def get_acquisition_info_df(self):\n",
    "        return self.__get_reformated_df__()\n",
    "\n",
    "    def save_dataframe_as_new_acquisition_dataframe(self):\n",
    "        self._output_paths = AcquisitionTableOutputPaths(self._table_infos)\n",
    "        self.__remove_possible_pre_existing_ml_table__(self._output_paths.output_file_name)\n",
    "        df_reformater = AcquisitionTableReformater(table_infos = self._table_infos, header_infos=self._header_infos, samples = self._samples, dataframe_already_preformated=False)\n",
    "        df_reformater.reformat_and_save_acquisition_data_frame(self._output_paths.output_file_name)\n",
    "\n",
    "    def update_ml_file_location_in_method_parameters_yaml(self):\n",
    "        method_params = load_method_parameters(self._table_infos._results_dir)\n",
    "        if self._output_paths == None:\n",
    "            raise Exception(\"output paths not initialized! This could be because no dataframe was saved before\")\n",
    "        method_params[self._output_paths.ml_file_accession_in_yaml] = self._output_paths.output_file_name\n",
    "        save_dict_as_yaml(method_params, self._output_paths.method_parameters_yaml_path)\n",
    "    \n",
    "    def __get_reformated_df__(self):\n",
    "        df_reformater = AcquisitionTableReformater(table_infos = self._table_infos, header_infos=self._header_infos, samples = self._samples, dataframe_already_preformated=True)\n",
    "        df = df_reformater.reformat_and_load_acquisition_data_frame()\n",
    "        return df.convert_dtypes()\n",
    "\n",
    "    def __reformat_samples_if_necessary(self, samples):\n",
    "        if \"plexDIA\" in  self._table_infos._input_type:\n",
    "            return self.__get_plexDIA_samplenames__(samples)\n",
    "        else:\n",
    "            return samples\n",
    "    \n",
    "    def __get_plexDIA_samplenames__(self, samples):\n",
    "        new_samples = []\n",
    "        for sample in samples:\n",
    "            new_samples.append(self.__get_samplename_without_mtraq_tag__(sample))\n",
    "        return new_samples\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_samplename_without_mtraq_tag__(samplename):\n",
    "        pattern = \"(.*)(_\\(mTRAQ-n-.\\))\"\n",
    "        matched = re.match(pattern, samplename)\n",
    "        return matched.group(1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __remove_possible_pre_existing_ml_table__(output_file_name):\n",
    "        if os.path.exists(output_file_name):\n",
    "            os.remove(output_file_name)\n",
    "            print(f\"removed pre existing {output_file_name}\")\n",
    "\n",
    "\n",
    "class AcquisitionTableInfo():\n",
    "    def __init__(self, results_dir, sep = \"\\t\", decimal = \".\"):\n",
    "        self._results_dir = results_dir\n",
    "        self._sep = sep\n",
    "        self._decimal = decimal\n",
    "        self._method_params_dict = load_method_parameters(results_dir)\n",
    "        self._input_file = self.__get_input_file__()\n",
    "        self._file_ending_of_formatted_table = \".ml_info_table.tsv\"\n",
    "        self.already_formatted =  self.__check_if_input_file_is_already_formatted__()\n",
    "        self._input_type, self._config_dict = self.__get_input_type_and_config_dict__()\n",
    "        self._sample_column = self.__get_sample_column__()\n",
    "        self.last_ion_level_to_use = self.__get_last_ion_level_to_use__()\n",
    "\n",
    "    def __get_input_file__(self):\n",
    "        if self._method_params_dict.get('ml_input_file') is None:\n",
    "            return self.__get_location_of_original_file__()\n",
    "        else:\n",
    "            return self._method_params_dict.get('ml_input_file')\n",
    "\n",
    "    def __check_if_input_file_is_already_formatted__(self):\n",
    "        if self._file_ending_of_formatted_table in self._input_file:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __get_input_type_and_config_dict__(self):\n",
    "        if self.already_formatted:\n",
    "            original_file = self.__get_location_of_original_file__()\n",
    "        else:\n",
    "            original_file = self._input_file\n",
    "        input_type, config_dict, _ = get_input_type_and_config_dict(original_file)\n",
    "        return input_type, config_dict\n",
    "    \n",
    "    def __get_location_of_original_file__(self):\n",
    "        input_file = self._method_params_dict.get('input_file')\n",
    "        return self.__get_original_filename_from_input_file__(input_file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_original_filename_from_input_file__(input_file):\n",
    "        pattern = \"(.*\\.tsv|.*\\.csv|.*\\.txt)(\\..*)(.aq_reformat.tsv)\"\n",
    "        m = re.match(pattern=pattern, string=input_file)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        else:\n",
    "            return input_file\n",
    "\n",
    "    \n",
    "    def __get_sample_column__(self):\n",
    "        return self._config_dict.get(\"sample_ID\")\n",
    "        \n",
    "    def __get_last_ion_level_to_use__(self):\n",
    "        return self._config_dict[\"ml_level\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AcquisitionTableHeaders():\n",
    "    def __init__(self, acquisition_table_info):\n",
    "\n",
    "        self._table_info = acquisition_table_info\n",
    "\n",
    "        self._ion_hierarchy = self.__get_ordered_ion_hierarchy__()\n",
    "        self._included_levelnames = self.__get_included_levelnames__()\n",
    "        self._ion_headers_grouped = self.__get_ion_headers_grouped__()\n",
    "        self._ion_headers = self.__get_ion_headers__()\n",
    "        self._numeric_headers = self.__get_numeric_headers__()\n",
    "        self._relevant_headers = self.__get_relevant_headers__()\n",
    "    \n",
    "    def __get_ordered_ion_hierarchy__(self):\n",
    "        ion_hierarchy = self._table_info._config_dict.get(\"ion_hierarchy\")\n",
    "        hier_key = 'fragion' if 'fragion' in ion_hierarchy.keys() else list(ion_hierarchy.keys())[0]\n",
    "        ion_hierarchy_on_chosen_key = ion_hierarchy.get(hier_key)\n",
    "        return ion_hierarchy_on_chosen_key\n",
    "\n",
    "    def __get_included_levelnames__(self):\n",
    "        levelnames = self.__get_all_levelnames__(self._ion_hierarchy)\n",
    "        last_ionlevel_idx = levelnames.index(self._table_info.last_ion_level_to_use)\n",
    "        return levelnames[:last_ionlevel_idx+1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_all_levelnames__(ion_hierarchy):\n",
    "        return  ion_hierarchy.get('order')\n",
    "\n",
    "    def __get_ion_headers_grouped__(self):\n",
    "        mapping_dict = self.__get_levelname_mapping_dict(self._ion_hierarchy)\n",
    "        return [mapping_dict.get(x) for x in self._included_levelnames]#on each level there can be multiple names, so it is a list of lists\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_levelname_mapping_dict(ion_hierarchy):\n",
    "        return ion_hierarchy.get('mapping')\n",
    "    \n",
    "    def __get_ion_headers__(self):\n",
    "        return list(itertools.chain(*self._ion_headers_grouped))\n",
    "\n",
    "    \n",
    "    def __get_relevant_headers__(self):\n",
    "        relevant_headers = self._numeric_headers+self._ion_headers + [self._table_info._sample_column]\n",
    "        return self.__remove_possible_none_values_from_list__(relevant_headers)\n",
    "    \n",
    "    @staticmethod\n",
    "    def __remove_possible_none_values_from_list__(list):\n",
    "        return [x for x in list if x is not None]\n",
    "\n",
    "    def __get_numeric_headers__(self):\n",
    "        df_sample = pd.read_csv(self._table_info._input_file, sep = self._table_info._sep, decimal = self._table_info._decimal, encoding='latin1', nrows=3000) #sample 3000 rows from the df to assess the types of each row\n",
    "        df_sample = df_sample.replace({False: 0, True: 1})\n",
    "        numeric_headers =  list(df_sample.select_dtypes(include=np.number).columns)\n",
    "        numeric_headers = AcquisitionTableHeaderFilter().filter_numeric_headers_if_specified(input_type = self._table_info._input_type, numeric_headers = numeric_headers)\n",
    "        return numeric_headers\n",
    "\n",
    "\n",
    "class AcquisitionTableOutputPaths():\n",
    "    def __init__(self, table_info):\n",
    "        self._table_info = table_info\n",
    "        self.output_file_name = self.__get_output_file_name__()\n",
    "        self.method_parameters_yaml_path = self.__get_method_parameters_yaml_path__()\n",
    "        self.ml_file_accession_in_yaml = \"ml_input_file\"\n",
    "\n",
    "    def __get_output_file_name__(self):\n",
    "        old_file_name = self._table_info._input_file\n",
    "        new_file_name = old_file_name+self._table_info._file_ending_of_formatted_table\n",
    "        return new_file_name\n",
    "\n",
    "    def __get_method_parameters_yaml_path__(self):\n",
    "        return f\"{self._table_info._results_dir}/aq_parameters.yaml\"\n",
    "\n",
    "\n",
    "class AcquisitionTableReformater(LongTableReformater):\n",
    "    def __init__(self, table_infos, header_infos, samples, dataframe_already_preformated = False):\n",
    "        \n",
    "        LongTableReformater.__init__(self, table_infos._input_file)\n",
    "        self._table_infos = table_infos\n",
    "        self._header_infos = header_infos\n",
    "        self._samples = samples\n",
    "        self._dataframe_already_preformated = dataframe_already_preformated\n",
    "\n",
    "        #set the two functions that specify the explicit reformatting\n",
    "        self._reformatting_function = self.__reformatting_function__\n",
    "        self._iterator_function = self.__initialize_iterator_with_specified_columns__\n",
    "    \n",
    "    def __reformatting_function__(self, input_df_subset):\n",
    "        input_df_subset = input_df_subset.drop_duplicates()\n",
    "        input_df_subset = self.__filter_reformated_df_if_necessary__(input_df_subset)\n",
    "        if not self._dataframe_already_preformated:\n",
    "            input_df_subset = add_merged_ionnames(input_df_subset, self._header_infos._included_levelnames, self._header_infos._ion_headers_grouped, None, None)\n",
    "        return input_df_subset\n",
    "\n",
    "    def __filter_reformated_df_if_necessary__(self, reformatted_df):\n",
    "        if 'spectronaut' in self._table_infos._input_type or 'diann' in self._table_infos._input_type:\n",
    "            return self.__filter_reformatted_dataframe_to_relevant_samples__(reformatted_df)\n",
    "        else:\n",
    "            return reformatted_df\n",
    "\n",
    "    def __filter_reformatted_dataframe_to_relevant_samples__(self, input_df_subset):\n",
    "        return input_df_subset[[x in self._samples for x in input_df_subset[self._table_infos._sample_column]]]\n",
    "    \n",
    "    def __initialize_iterator_with_specified_columns__(self):\n",
    "        cols_to_use = self.__get_cols_to_use__()\n",
    "        return pd.read_csv(self._table_infos._input_file, sep = self._table_infos._sep, decimal=self._table_infos._decimal, usecols = cols_to_use, encoding ='latin1', chunksize=1000000)\n",
    "\n",
    "    def __get_cols_to_use__(self):\n",
    "        cols_to_use = self._header_infos._relevant_headers\n",
    "        if self._dataframe_already_preformated:\n",
    "            return cols_to_use+['ion']\n",
    "        else:\n",
    "            return cols_to_use\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AcquisitionTableHeaderFilter():\n",
    "    def __init__(self):\n",
    "        self._spectronaut_header_filter = lambda x : ((\"EG.\" in x) | (\"FG.\" in x)) and (\"Global\" not in x)\n",
    "        self._maxquant_header_filter = lambda x : (\"Intensity\" not in x) and (\"Experiment\" not in x)\n",
    "\n",
    "    def filter_numeric_headers_if_specified(self, input_type, numeric_headers):\n",
    "        if 'spectronaut' in input_type:\n",
    "            return [x for x in numeric_headers if self._spectronaut_header_filter(x)]\n",
    "        elif 'maxquant' in input_type:\n",
    "            return [x for x in numeric_headers if self._maxquant_header_filter(x)]\n",
    "        else:\n",
    "            return numeric_headers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def merge_acquisition_df_parameter_df(acquisition_df, parameter_df, groupby_merge_type = 'mean'):\n",
    "    \"\"\"acquisition df contains details on the acquisition, parameter df are the parameters derived from the tree\n",
    "    \"\"\"\n",
    "    merged_df = parameter_df.merge(acquisition_df, how = 'left', on = 'ion')\n",
    "    if groupby_merge_type == 'mean':\n",
    "        merged_df = merged_df.groupby('ion').mean().reset_index()\n",
    "    if groupby_merge_type == 'min':\n",
    "        merged_df = merged_df.groupby('ion').min().reset_index()\n",
    "    if groupby_merge_type == 'max':\n",
    "        merged_df = merged_df.groupby('ion').max().reset_index()\n",
    "    merged_df = merged_df.dropna(axis=1, how='all')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import itertools\n",
    "\n",
    "def plot_withincond_fcs(normed_intensity_df, cut_extremes = True):\n",
    "    \"\"\"takes a normalized intensity dataframe and plots the fold change distribution between all samples. Column = sample, row = ion\"\"\"\n",
    "\n",
    "    samplecombs = list(itertools.combinations(normed_intensity_df.columns, 2))\n",
    "\n",
    "    for spair in samplecombs:#compare all pairs of samples\n",
    "        s1 = spair[0]\n",
    "        s2 = spair[1]\n",
    "        diff_fcs = normed_intensity_df[s1].to_numpy() - normed_intensity_df[s2].to_numpy() #calculate fold changes by subtracting log2 intensities of both samples\n",
    "\n",
    "        if cut_extremes:\n",
    "            cutoff = max(abs(np.nanquantile(diff_fcs,0.025)), abs(np.nanquantile(diff_fcs, 0.975))) #determine 2.5% - 97.5% interval, i.e. remove extremes\n",
    "            range = (-cutoff, cutoff)\n",
    "        else:\n",
    "            range = None\n",
    "        plt.hist(diff_fcs,80,density=True, histtype='step',range=range) #set the cutoffs to focus the visualization\n",
    "        plt.xlabel(\"log2 peptide fcs\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import itertools\n",
    "\n",
    "def plot_relative_to_median_fcs(normed_intensity_df):\n",
    "\n",
    "    median_intensities = normed_intensity_df.median(axis=1)\n",
    "    median_intensities = median_intensities.to_numpy()\n",
    "    \n",
    "    diff_fcs = []\n",
    "    for col in normed_intensity_df.columns:\n",
    "        median_fcs = normed_intensity_df[col].to_numpy() - median_intensities\n",
    "        diff_fcs.append(np.nanmedian(median_fcs))\n",
    "    plt.hist(diff_fcs,80,density=True, histtype='step')\n",
    "    plt.xlabel(\"log2 peptide fcs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the in-memory and out-of-memory longformat table loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/constantin/workspace/directlfq/nbdev_nbs\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import matplotlib_venn\n",
    "import directlfq.utils as lfqutils\n",
    "\n",
    "print(os.path.abspath(\".\"))\n",
    "input_file = \"../test_data/unit_tests/input_table_formats/spectronaut.frgions.large.tsv\"\n",
    "outdir = \"../test_data/unit_tests/input_table_formats/loading_comparisons\"\n",
    "\n",
    "file_default = \"default_out.tsv\"\n",
    "file_dask_proc = \"dask_proc_out.tsv\"\n",
    "\n",
    "def test_table_loadings(input_file, outdir, file_default, file_dask_proc):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "        #os.chdir(outdir)\n",
    "\n",
    "    input_type, config_dict_for_type, sep = lfqutils.get_input_type_and_config_dict(input_file)\n",
    "\n",
    "    lfqutils.reformat_and_write_longtable_according_to_config(input_file,outfile_name=file_dask_proc,config_dict_for_type=config_dict_for_type, enforce_largefile_processing=True, chunksize=10_000)\n",
    "    lfqutils.reformat_and_write_longtable_according_to_config(input_file,outfile_name=file_default,config_dict_for_type=config_dict_for_type, chunksize=10_000)\n",
    "\n",
    "    df_default = pd.read_csv(file_default, sep = \"\\t\")\n",
    "\n",
    "    df_dask_proc = pd.read_csv(file_dask_proc, sep = \"\\t\")\n",
    "\n",
    "\n",
    "    assert df_default.equals(df_dask_proc)\n",
    "\n",
    "    matplotlib_venn.venn2([set(df_default[\"ion\"]), set(df_dask_proc[\"ion\"])])\n",
    "\n",
    "    os.remove(file_default)\n",
    "    os.remove(file_dask_proc)\n",
    "    shutil.rmtree(outdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_table_loadings(input_file, outdir, file_default, file_dask_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using input type diann_fragion_isotopes\n",
      "loading ran through\n",
      "using input type spectronaut_precursor_v2\n",
      "loading ran through\n",
      "using input type spectronaut_fragion_isotopes\n",
      "loading ran through\n",
      "using input type maxquant_peptides_leading_razor_protein\n",
      "loading ran through\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "\n",
    "import directlfq.utils as lfqutils\n",
    "import os\n",
    "import shutil\n",
    "outdir = \"../test_data/unit_tests/input_table_formats/\"\n",
    "\n",
    "tabledir =\"../test_data/unit_tests/input_table_formats/\"\n",
    "results_dir = \"../test_data/unit_tests/results/\"\n",
    "\n",
    "\n",
    "input_files = [os.path.join(tabledir, x ) for x in [\"diann.tsv\", \"spectronaut.tsv\", \"spectronaut_frgion.tsv\", \"mq_peptides.txt\"]]\n",
    "samplemap_files = [os.path.join(tabledir, x ) for x in [\"samplemap.diann.tsv\", \"samplemap.spectronaut.tsv\", \"samplemap.spectronaut.frgions.tsv\", \"samplemap.mq.tsv\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perform_table_loading(input_file, samplemap_file):\n",
    "    \"\"\"only makes sure that the commands run without error\"\"\"\n",
    "    \n",
    "    #import the input table once the input and the results folder are specified. \n",
    "    # The function automatically recognizes the format (Currently MQ, Spectronaut, DIA-NN configured)\n",
    "    input_data = lfqutils.import_data(input_file)\n",
    "    #display(input_data)\n",
    "    \n",
    "    #get sample names from the imported table\n",
    "    samplenames = lfqutils.get_samplenames_from_input_df(input_data)\n",
    "    #display(samplenames)\n",
    "\n",
    "    #load the samplemap dataframe (in case the user uploads a file. Basically a pandas import + separator check)\n",
    "    samplemap_df = lfqutils.load_samplemap(samplemap_file)\n",
    "    #display(samplemap_df)\n",
    "\n",
    "    #compare samplemap and actual table, merge & logtransform intensities\n",
    "    input_processed, samplemap_df_processed = lfqutils.prepare_loaded_tables(input_data, samplemap_df)\n",
    "    #display(input_processed)\n",
    "    #display(samplemap_df_processed)\n",
    "\n",
    "for idx in range(len(input_files)):\n",
    "    perform_table_loading(input_files[idx], samplemap_files[idx])\n",
    "    print('loading ran through')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib_venn import venn2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_generic_table_with_original(preprocessed_input_df, original_input_file, config_yaml,input_typename_config, sep = \"\\t\"):\n",
    "    id2quant_orig, id2quant_preproc = get_processed_original_id2quant_maps(preprocessed_input_df, original_input_file, config_yaml,input_typename_config)\n",
    "    keys_orig = set(id2quant_orig.keys())\n",
    "    keys_preproc = set(id2quant_preproc.keys())\n",
    "    keydiff = keys_preproc.difference(keys_orig)\n",
    "    keys_orig = sorted(keys_orig)\n",
    "    keys_preproc = sorted(keys_preproc)\n",
    "    print(list(keys_orig)[:10])\n",
    "    print(list(keys_preproc)[:10])\n",
    "    \n",
    "    \n",
    "    assert(len(keydiff)==0) #check that all keys in the preprocessed set are part of the original set\n",
    "\n",
    "    venn2([set(id2quant_orig.keys()), set(id2quant_preproc.keys())], [\"original\", \"preprocessed\"])\n",
    "    \n",
    "    quantvec_orig = np.array([id2quant_orig.get(x)for x in id2quant_preproc.keys()])\n",
    "    quantvec_preproc = np.array([id2quant_preproc.get(x)for x in id2quant_preproc.keys()])\n",
    "    unequal_quant = [id2quant_orig.get(x)!=id2quant_preproc.get(x) for x in id2quant_preproc.keys()]\n",
    "    unequal_quant_scaled = [id2quant_orig.get(x)*10000!=id2quant_preproc.get(x) for x in id2quant_preproc.keys()]\n",
    "    print(sum(unequal_quant))\n",
    "    print(sum(unequal_quant_scaled))\n",
    "    plt.show()\n",
    "    plt.scatter(quantvec_orig, quantvec_preproc)\n",
    "    plt.show()\n",
    "    corrcoeff = np.corrcoef(quantvec_orig,quantvec_preproc)[0][1]\n",
    "    print(f\"correlation between both processings: {corrcoeff}\")\n",
    "    assert(corrcoeff>0.999)\n",
    "\n",
    "\n",
    "\n",
    "def get_processed_original_id2quant_maps(preprocessed_input_df, original_input_file, config_yaml,input_typename_config, sep = \"\\t\"):\n",
    "    config_all = yaml.safe_load(open(config_yaml, 'r'))\n",
    "    config_dict = config_all.get(input_typename_config)\n",
    "    id_cols = config_dict.get(\"ion_cols\") + [config_dict.get(\"sample_ID\")]\n",
    "    quant_col = list(config_dict.get(\"quant_ID\").values())\n",
    "    id2quant_orig = get_id2quant_original(original_input_file, id_cols, quant_col, sep)\n",
    "    id2quant_preproc = get_id2quant_processed(preprocessed_input_df, id_cols, quant_col)\n",
    "    return id2quant_orig, id2quant_preproc\n",
    "\n",
    "\n",
    "def get_id2quant_original(original_input_file, id_cols, quant_col, sep):\n",
    "    print(id_cols)\n",
    "    print(id_cols+quant_col)\n",
    "    orig_df = pd.read_csv(original_input_file, sep=sep, usecols= id_cols+quant_col)\n",
    "    orig_df[\"compareID\"] = orig_df[id_cols].astype('string').sum(axis = 1)\n",
    "    display(orig_df)\n",
    "    id2quant = dict(zip(orig_df[\"compareID\"], orig_df[quant_col[0]]))\n",
    "    id2quant = {k: round(v,3) for k, v in id2quant.items()}\n",
    "    return id2quant\n",
    "\n",
    "\n",
    "def get_id2quant_processed(preprocessed_input_df, id_cols, quant_col):\n",
    "    compare_IDs = []\n",
    "    quantvals = []\n",
    "    for column in preprocessed_input_df.columns:\n",
    "        if(column == \"protein\"):\n",
    "            continue\n",
    "        id = pd.Series([column for x in range(len(preprocessed_input_df.index))]).to_numpy()[0]\n",
    "        reformated_pep_id = [x.split(\"_MOD_\")[1].replace(\"_CHARGE_\", \"\")[:-1]+id for x in preprocessed_input_df.index]\n",
    "        compare_IDs.extend(reformated_pep_id)\n",
    "\n",
    "        quantvals.extend(list(preprocessed_input_df[column]))\n",
    "    \n",
    "    id2quant = dict(zip(compare_IDs, quantvals))\n",
    "    id2quant = {k: round(2**v,3) for k, v in id2quant.items() if ~np.isnan(v)}\n",
    "    return id2quant\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using input type spectronaut_precursor_v2\n",
      "['EG.ModifiedPeptide', 'FG.Charge', 'R.Label']\n",
      "['EG.ModifiedPeptide', 'FG.Charge', 'R.Label', 'FG.Quantity']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R.Label</th>\n",
       "      <th>EG.ModifiedPeptide</th>\n",
       "      <th>FG.Charge</th>\n",
       "      <th>FG.Quantity</th>\n",
       "      <th>compareID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_LNVLPVDVLTR_</td>\n",
       "      <td>2</td>\n",
       "      <td>95427.601562</td>\n",
       "      <td>_LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_</td>\n",
       "      <td>2</td>\n",
       "      <td>110807.851562</td>\n",
       "      <td>_GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          R.Label  \\\n",
       "0     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "1     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "2     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "3     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "4     E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "...                                           ...   \n",
       "9994  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9995  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9996  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9997  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "9998  E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms   \n",
       "\n",
       "                    EG.ModifiedPeptide  FG.Charge    FG.Quantity  \\\n",
       "0                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "1                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "2                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "3                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "4                        _LNVLPVDVLTR_          2   95427.601562   \n",
       "...                                ...        ...            ...   \n",
       "9994  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9995  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9996  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9997  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "9998  _GC[Carbamidomethyl (C)]VITISGR_          2  110807.851562   \n",
       "\n",
       "                                              compareID  \n",
       "0     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "1     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "2     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "3     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "4     _LNVLPVDVLTR_2E_D170331_S209-S-1-240min_MHRM_R...  \n",
       "...                                                 ...  \n",
       "9994  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9995  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9996  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9997  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "9998  _GC[Carbamidomethyl (C)]VITISGR_2E_D170331_S20...  \n",
       "\n",
       "[9999 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_AAAAASAAGPGGLVAGKEEK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGKEEK_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAANFSDR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAPEKPDAEHDAPQFIEPLDSIDR_4E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAFDVIVR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGAILK_1E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGEHIASSGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGIWYEHR_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AALAATGAASGGGGGGGGAGSR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms']\n",
      "['_AAAAASAAGPGGLVAGKEEK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGKEEK_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAAASAAGPGGLVAGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAANFSDR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAAPEKPDAEHDAPQFIEPLDSIDR_4E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAFDVIVR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGAILK_1E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGEHIASSGK_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AAGIWYEHR_3E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms', '_AALAATGAASGGGGGGGGAGSR_2E_D170331_S209-S-1-240min_MHRM_R01_T0.htrms']\n",
      "49\n",
      "1716\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation between both processings: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = os.path.join(tabledir, \"spectronaut.tsv\")\n",
    "samplemap_file = os.path.join(tabledir, \"samplemap.spectronaut.tsv\")\n",
    "\n",
    "\n",
    "input_data = lfqutils.import_data(input_file)\n",
    "samplemap_df = lfqutils.load_samplemap(samplemap_file)\n",
    "input_processed, samplemap_df_processed = lfqutils.prepare_loaded_tables(input_data, samplemap_df)\n",
    "compare_generic_table_with_original(input_processed, input_file, \"../directlfq/configs/intable_config.yaml\", \"spectronaut_precursor_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphatemplate",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
